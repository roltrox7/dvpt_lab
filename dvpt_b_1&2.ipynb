{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3717c788-f1c6-4411-95fd-7b4f5090a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 12:15:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appNxame(\"DataFrame_Operations\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e61f48-023d-4c9a-b08e-1697c18c19ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+---+------+\n",
      "|   name|        city|age|gender|\n",
      "+-------+------------+---+------+\n",
      "|  Alice|    New York| 25|Female|\n",
      "|    Bob| Los Angeles| 30|  Male|\n",
      "|Charlie|     Chicago| 22|  Male|\n",
      "|  Diana|     Houston| 28|Female|\n",
      "|    Eve|     Phoenix| 35|Female|\n",
      "|  Frank|Philadelphia| 40|  Male|\n",
      "|  Grace| San Antonio| 29|Female|\n",
      "|   Hank|   San Diego| 33|  Male|\n",
      "|    Ivy|      Dallas| 31|Female|\n",
      "|   Jack|    San Jose| 27|  Male|\n",
      "+-------+------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data for the DataFrame\n",
    "data = [\n",
    "    (\"Alice\", \"New York\", 25, \"Female\"),\n",
    "    (\"Bob\", \"Los Angeles\", 30, \"Male\"),\n",
    "    (\"Charlie\", \"Chicago\", 22, \"Male\"),\n",
    "    (\"Diana\", \"Houston\", 28, \"Female\"),\n",
    "    (\"Eve\", \"Phoenix\", 35, \"Female\"),\n",
    "    (\"Frank\", \"Philadelphia\", 40, \"Male\"),\n",
    "    (\"Grace\", \"San Antonio\", 29, \"Female\"),\n",
    "    (\"Hank\", \"San Diego\", 33, \"Male\"),\n",
    "    (\"Ivy\", \"Dallas\", 31, \"Female\"),\n",
    "    (\"Jack\", \"San Jose\", 27, \"Male\"),\n",
    "]\n",
    "\n",
    "# Define the schema (column names)\n",
    "columns = [\"name\", \"city\", \"age\", \"gender\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5be729-dca6-4f8a-8f9a-e49c16064f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+------+\n",
      "| name|        city|age|gender|\n",
      "+-----+------------+---+------+\n",
      "|  Eve|     Phoenix| 35|Female|\n",
      "|Frank|Philadelphia| 40|  Male|\n",
      "| Hank|   San Diego| 33|  Male|\n",
      "|  Ivy|      Dallas| 31|Female|\n",
      "+-----+------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where age > 30\n",
    "filtered_df = df.filter(df.age > 30)\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5a34ca3-8317-4769-84c2-f6cc1326d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"tax\", col(\"age\") * 0.1)\n",
    "\n",
    "df = df.withColumnRenamed(\"age\", \"years\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b895f0b-047f-4875-a39c-3fb09b8a1e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|   name|years|               tax|\n",
      "+-------+-----+------------------+\n",
      "|  Alice|   25|               2.5|\n",
      "|    Bob|   30|               3.0|\n",
      "|Charlie|   22|               2.2|\n",
      "|  Diana|   28|2.8000000000000003|\n",
      "|    Eve|   35|               3.5|\n",
      "|  Frank|   40|               4.0|\n",
      "|  Grace|   29|2.9000000000000004|\n",
      "|   Hank|   33|3.3000000000000003|\n",
      "|    Ivy|   31|               3.1|\n",
      "|   Jack|   27|               2.7|\n",
      "+-------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.drop(\"city\", \"gender\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f15ccbfe-862a-4da1-a867-6385a24415bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "+---------+------------+-----------+-----+-------------+------+\n",
      "|ProductID| ProductName|   Category|Price|StockQuantity|Rating|\n",
      "+---------+------------+-----------+-----+-------------+------+\n",
      "|      101|      Laptop|Electronics|800.0|           50|   4.5|\n",
      "|      102|  Headphones|Electronics| 50.0|          200|   4.3|\n",
      "|      103|  Smartphone|Electronics|500.0|           80|   4.6|\n",
      "|      104|        Sofa|  Furniture|300.0|           20|   4.1|\n",
      "|      105|Dining Table|  Furniture|400.0|           15|   4.0|\n",
      "|      106|       Chair|  Furniture| 50.0|          100|   4.2|\n",
      "|      107|    Notebook| Stationery|  5.0|          500|   4.8|\n",
      "|      108|         Pen| Stationery|  2.0|         1000|   4.9|\n",
      "|      109|     Printer|Electronics|150.0|           30|   4.0|\n",
      "|      110|         Bed|  Furniture|700.0|           10|   4.3|\n",
      "+---------+------------+-----------+-----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (101, \"Laptop\", \"Electronics\", 800, 50, 4.5),\n",
    "    (102, \"Headphones\", \"Electronics\", 50, 200, 4.3),\n",
    "    (103, \"Smartphone\", \"Electronics\", 500, 80, 4.6),\n",
    "    (104, \"Sofa\", \"Furniture\", 300, 20, 4.1),\n",
    "    (105, \"Dining Table\", \"Furniture\", 400, 15, 4.0),\n",
    "    (106, \"Chair\", \"Furniture\", 50, 100, 4.2),\n",
    "    (107, \"Notebook\", \"Stationery\", 5, 500, 4.8),\n",
    "    (108, \"Pen\", \"Stationery\", 2, 1000, 4.9),\n",
    "    (109, \"Printer\", \"Electronics\", 150, 30, 4.0),\n",
    "    (110, \"Bed\", \"Furniture\", 700, 10, 4.3)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"ProductID\", \"ProductName\", \"Category\", \"Price\", \"StockQuantity\", \"Rating\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Ensure correct data types for Price and StockQuantity\n",
    "df = df.withColumn(\"Price\", col(\"Price\").cast(\"float\"))\n",
    "df = df.withColumn(\"StockQuantity\", col(\"StockQuantity\").cast(\"int\"))\n",
    "\n",
    "# Show initial DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a592f96-cc51-457e-9b3d-1fae68d1c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted DataFrame (by Price descending, then by Category ascending):\n",
      "+---------+------------+-----------+-----+-------------+------+\n",
      "|ProductID| ProductName|   Category|Price|StockQuantity|Rating|\n",
      "+---------+------------+-----------+-----+-------------+------+\n",
      "|      101|      Laptop|Electronics|800.0|           50|   4.5|\n",
      "|      110|         Bed|  Furniture|700.0|           10|   4.3|\n",
      "|      103|  Smartphone|Electronics|500.0|           80|   4.6|\n",
      "|      105|Dining Table|  Furniture|400.0|           15|   4.0|\n",
      "|      104|        Sofa|  Furniture|300.0|           20|   4.1|\n",
      "|      109|     Printer|Electronics|150.0|           30|   4.0|\n",
      "|      102|  Headphones|Electronics| 50.0|          200|   4.3|\n",
      "|      106|       Chair|  Furniture| 50.0|          100|   4.2|\n",
      "|      107|    Notebook| Stationery|  5.0|          500|   4.8|\n",
      "|      108|         Pen| Stationery|  2.0|         1000|   4.9|\n",
      "+---------+------------+-----------+-----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame\n",
    "sorted_df = df.orderBy(col(\"Price\").desc(), col(\"Category\").asc())\n",
    "print(\"Sorted DataFrame (by Price descending, then by Category ascending):\")\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a53ded0-3696-49e9-8fd0-a6176cf34c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|   Category|TotalSalesByCategory|\n",
      "+-----------+--------------------+\n",
      "|Electronics|             94500.0|\n",
      "|  Furniture|             24000.0|\n",
      "| Stationery|              4500.0|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Add a column for total sales (Price * StockQuantity)\n",
    "sales_df = df.withColumn(\"TotalSales\", col(\"Price\") * col(\"StockQuantity\"))\n",
    "\n",
    "# Group by Category and calculate total sales\n",
    "sales_by_category = sales_df.groupBy(\"Category\").agg(sum(col(\"TotalSales\")).alias(\"TotalSalesByCategory\"))\n",
    "\n",
    "# Show the result\n",
    "sales_by_category.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de100d35-a9f4-426d-b80b-a26db42cbc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-----------------+\n",
      "| ProductName|TotalSalesAmount|TotalQuantitySold|\n",
      "+------------+----------------+-----------------+\n",
      "|      Laptop|         40000.0|               50|\n",
      "|  Headphones|         10000.0|              200|\n",
      "|  Smartphone|         40000.0|               80|\n",
      "|        Sofa|          6000.0|               20|\n",
      "|Dining Table|          6000.0|               15|\n",
      "|       Chair|          5000.0|              100|\n",
      "|    Notebook|          2500.0|              500|\n",
      "|         Pen|          2000.0|             1000|\n",
      "|         Bed|          7000.0|               10|\n",
      "|     Printer|          4500.0|               30|\n",
      "+------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Group by ProductName and calculate total sales and quantity sold\n",
    "sales_by_product = sales_df.groupBy(\"ProductName\").agg(\n",
    "    sum(\"TotalSales\").alias(\"TotalSalesAmount\"),\n",
    "    sum(\"StockQuantity\").alias(\"TotalQuantitySold\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "sales_by_product.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b9dfc-e6ee-4136-8d72-2ee6179d5584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
